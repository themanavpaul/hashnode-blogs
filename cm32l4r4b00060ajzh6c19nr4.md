---
title: "Day 4 : Hands-On Data Preprocessing | Generative AI"
seoTitle: "Data Preprocessing : Day 4 of Master Generative AI"
seoDescription: "Learn hands-on data preprocessing techniques for generative AI. Includes steps for lowercasing, HTML tag removal, URL handling, and more"
datePublished: Mon Nov 04 2024 05:33:03 GMT+0000 (Coordinated Universal Time)
cuid: cm32l4r4b00060ajzh6c19nr4
slug: day-4-hands-on-data-preprocessing-generative-ai
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1730262784168/36b8390d-dfb4-460d-846f-1fcc586d1ee8.png
tags: 100daysofcode, 30-days-of-code, llm, wemakedevs, generative-ai, genai, 100xdevs, mastergenai

---

üëãüèª Welcome back to my blog! If you're new here, I recommend starting with [**Day 1 : Introduction to GenAI**](https://manavpaul.hashnode.dev/day-1-introduction-to-generative-ai) to get a solid foundation in generative AI.  
On **Day 2** and **3**, we cover all the steps to make an end to end generative ai pipeline. *Today*, we'll get our hands dirty on the practical aspects of data preprocessing techniques.

**Full Playlist** : **‚ñ∂** [**Master GenAI Series**](https://manavpaul.hashnode.dev/series/generative-ai)

üòä Hello World, I'm **Manav Paul**, a 24-year-young **spiritual developer** on a mission to understand and leverage the power of generative AI. This blog serves as a roadmap, guiding you through my 30-day journey of discovering the fascinating world of generative AI. Let's continue our exploration together!

---

Here we will be learning how to clean up the data using the data preprocessing techniques.  
Let‚Äôs get our hands dirty on some code.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730456931015/fd92498e-2457-480a-991d-217e2bf5c9f1.gif align="center")

First, we need a dataset to work on. So let‚Äôs log in to [Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) and download our [**IMDB** dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) having *50K movie reviews* for natural language processing or Text analytics.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730263271398/8e2e1c4e-4a32-4476-800d-7aac83b9dde5.jpeg align="center")

After downloading and extracting the zip file, lets head to [Google Collab](https://colab.research.google.com/) and create a notebook. Though I will be providing a notebook for easy understanding but I‚Äôll recommend one must try it own their on for better hands on approach.  
Below I have created a notebook ‚Äú[***data\_preprocessing.ipynb***](https://github.com/themanavpaul/hashnode-blogs/blob/main/data_preprocess_notebook.ipynb)‚Äù. Upload your dataset ‚Äú***IMDB dataset.csv***‚Äù to the notebook by right clicking inside ***Files*** *folder* option in navbar on the left.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730263829590/9d67f03c-b518-482f-bd9d-6aa60b4d8b1c.png align="center")

Execute the following commands in notebook to load the dataset and show.

```python
# Pandas help load CSV file
import pandas as pd
# Check your present working directory 
!pwd
# set your file path to the current working directory
data_path = "/content/IMDB Dataset.csv"
# Load the data ‚¨á
df = pd.read_csv(data_path)
# Shape of the data -> 50k reviews with 2 columns - reviews ,sentiment
df.shape
# Show the data
df.head()
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730265330891/b16f115e-21af-422f-91b6-9935a9ee923b.png align="center")

Everything is set up and now you are ready to begin with. Let‚Äôs Go!

Since making operation on 50k reviews will consume a lot of time and memory, we will operate on first 100 reviews only.

```python
# Load first 100 Examples
df = df.head(100)
# Shape will be (100,2)
df.shape
# Show the data
df.head()
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730265570558/b2c0f8b0-fd77-419f-8605-1de17275149e.png align="center")

---

# Lowercase Data

On day 2, we did Data preprocessing technique in which we learned about lowercasing which is to convert all text to lowercase to standardize the data.

```python
# Take row 3 in review column, Notice some uppercase characters.
df['review'][3]
# convert everything to string then lower() operation on review column 
df['review'] = df['review'].str.lower()
# show all the data
df
# show our new Row 3 with all lowercase
df['review'][3]
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730266100526/ef64f55d-f5ba-439e-9f0e-f53ae84f6468.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730266143136/a831c982-32f3-40ee-bec2-a7d946c065b2.png align="center")

---

# Remove &lt;HTML&gt; tags

Now, we will practice how to remove the html tags in data preprocessing.

Let‚Äôs create a function named `remove_html_tags`.  
Used **Regular Expression** with pattern of `html` tags and replace it with `'empty'`

```python
import re
def remove_html_tags(text):
  pattern = re.compile('<.*?>')
  return pattern.sub(r'', text)
```

> Example:  
> Manav is a *Yoga Instructor.*  
> *This is a* ***Gen Ai*** *tutorial*

```python
text = "<html><body><p> Manav is a <i>Yoga<i> Instructor.</p><p>This is a <b>Gen Ai</b> tutorial</p><a href='http://google.com'></body></html>"
```

Now, remove the html tags using

```python
remove_html_tags(text)
# Output
#  Manav is a Yoga Instructor.This is a Gen Ai tutorial
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730269890806/3f16a70e-6739-4b1e-b5aa-2ce1376bb541.png align="center")

Similarly, we can apply the same function to our dataset

```python
df['review'] = df['review'].apply(remove_html_tags)
```

This will remove all the html tags in our dataset. Let‚Äôs check row number 5.

```python
df['review'][5]
# Output
# probably my all-time favorite movie,....they\'d all be "up" for this movie.
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730269947642/9478e2bc-7f4c-4c2d-9094-9ddebd23bd50.png align="center")

---

# Remove URLs

Similarly, we‚Äôll create a function named `remove_url`

```python
# prompt: remove_url function

def remove_url(text):
  pattern = re.compile(r'https?://\S+|www\.\S+')
  return pattern.sub(r'', text)

# Example usage:
text_with_url = "Check out this link: https://www.google.com"
cleaned_text = remove_url(text_with_url)
print(cleaned_text)  
# Output: Check out this link:
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730271141878/7ff2b91b-0e15-48c3-91ca-e884abfebccf.png align="center")

<div data-node-type="callout">
<div data-node-type="callout-emoji">üí°</div>
<div data-node-type="callout-text">It is not necessary to remove the url, only remove when you don‚Äôt want your data to have any.</div>
</div>

---

# Punctuation handling

Now let us understand how to remove the punctuations.

```python
import string,time
# to see all types of punctuations
string.punctuation
# that is
# !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~
```

```python
# all the string punctuations inside the exclude variable
exclude = string.punctuation
exclude
```

Create a function named `remove_punctuations`

```python
def remove_punctuations(text):
  for char in exclude:
    text = text.replace(char, '')
  return text
```

```python
text = 'Kaffee bitte, danke!'
```

```python
# We will check how much time does it take.
start = time.time()
print(remove_punctuations(text))
# caluclate the time to remove punctutation
time1 = time.time() - start
print(time1*50000)
# output:
# Kaffee bitte danke
# 10.216236114501953
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730272759896/0c38b3e6-246a-4273-a9b2-d4bbaf21b8dd.png align="center")

It can also be done using another method with less time as in the function `remove_punctuations` for loop is used thus resulting in linear time.

Create another function named `remove_punct`

```python
def remove_punct(text):
  return text.translate(str.maketrans('','', exclude))
```

```python
start = time.time()
print(remove_punct(text))
# caluclate the time to remove punctutation
time2 = time.time() - start
print(time2*50000)
# Kaffee bitte danke
# 9.393692016601562
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730272777447/7bdf43ec-b324-4f96-b05c-eb407ee0d545.png align="center")

Great, now apply the same function on the dataset and check the result.

```python
df['review'][5] # row number 5
remove_punct(df['review'][5]) # removing punctuations from row 5
# remove_punct(df['review'])  # Pass the entire dataset
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730273145315/7ab70142-2060-43fe-9ccb-53ad24b8c71c.png align="center")

---

# Chat Conversion Handle

Now, it time to handle the chat conversion written in short. Like the example below is the dictionary of such words

```python
chat_words = {
    'AFAIK':'As Far As I Know',
    'AFK':'Away From Keyboard',
    'ASAP':'As Soon As Possible',
    'ATK':'At The Keyboard',
    'CUL8R': 'See You Later',
    'ATM':'At The Moment'
}
{ # Some other example to make in dictionary
    "FYI: For Your Information",
    "ASAP: As Soon As Possible",
    "BRB: Be Right Back",
    "FAQ: Frequently Asked Questions",
    "IMBD: Internet Movie Database",
    "LOL: Laugh Out Loud",
    "BTW: By The Way",
    "CU: See You",
    "CUL8R: See You Later",
    "CYA: See You Again",
    "TTYL: Talk to Youb later"

}
```

Create a function named `chat_conversion` and check if the chat words are there in our dictionary.

```python
def chat_conversion(text):
  new_text = []
  for w in text.split():
    if w.upper() in chat_words:
      new_text.append(chat_words[w.upper()])
    else:
      new_text.append(w)
  return " ".join(new_text)
```

Let‚Äôs test this function

```python
chat_conversion("Ok, I am going. CUL8R")
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730274357121/d6483e47-88d3-419a-a0e3-3b8224250326.png align="center")

---

# Incorrect Text Handling

Sometime there could be mistakes during a real time conversations, so to handle these we will use `TextBlob`

```python
from textblob import TextBlob
# Incorrect Text
incorrect_text = "ceertain conditionas duriing seveal ggenerations aree moodified in the same maner."
textBlb = TextBlob(incorrect_text)
textBlb.correct().string
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730274896713/7b9ff73b-01ba-441c-8e8f-1c126c32263f.png align="center")

---

# Stopwords

You should remove these tokens only if they don't add any new information for your problem.

```python
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
```

```python
# List down all the stopwords in English
stopwords.words('english')
len(stopwords.words("english")) # 179 total
```

***So how does it really works?***

> Example :
> 
> Sentence: I **practice Yoga** almost **everyday**. I **love** Yoga.
> 
> We can understand the sentiment of the sentence by the words `practice, yoga, everyday, love`
> 
> Rest of the words are the stopwords which when removed the sentiment of the text remains the same. Hence reducing dimensionality.

Let‚Äôs test this function

```python
def remove_stopwards(text):
  new_text = []
  for word in text.split():
    if word in stopwords.words('english'):
      new_text.append('')
    else:
      new_text.append(word)
  x = new_text[:]
  new_text.clear()
  return " ".join(x)
```

Test it on any row or the entire dataset you want like I did on Row number 5.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730276086370/c168938b-abd9-4f56-8b38-735743e6a418.png align="center")

```python
# To apply on the entire dataset (demo)
df["review"].apply(remove_stopwords)
# To permanent apply on the dataset
df["review"] = df["review"].apply(remove_stopwords)
```

---

# Emoji Handling

Emojis are [uni-code characters](https://unicode.org/emoji/charts/full-emoji-list.html).

```python
import re
def remove_emoji(text):
  emoji_pattern = re.compile("["
                              u"\U0001F600-\U0001F64F"  # emoticons
                              u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                              u"\U0001F680-\U0001F6FF"  #transport and map symbols
                              u"\U0001F1E0-\U0001F1FF"  #flags
                              u"\U00002702-\U000027B0"
                              u"\U000024C2-\U0001F251"
                             "]+",flags=re.UNICODE)
  return emoji_pattern.sub(r'', text)
```

```python
remove_emoji("I love yoga. GenAI is üòçüòò")
# Output : I love yoga. GenAI is
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730276809034/90be5f5e-42f6-4d15-916c-ad048d5b73e5.png align="center")

But what if emojis are required. Like when you type an emoji in Chatgpt or Gemini they tend to know the meaning of it. How can we achieve that?

```python
!pip install emoji
```

*demojize* : extract the meaning of emojis

```python
import emoji
print(emoji.demojize("I ‚ô• Yoga, üßòüèª‚Äç‚ôÇÔ∏è is amazing. Lots of ü•∞"))
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730279660432/799b4ded-3e14-48de-bd00-35338dec621d.png align="center")

---

# Tokenization

### 1\. Using ***split( )*** function

1. Word Tokenization
    

```python
# word tokenisation
sent1 = "I am going to Mumbai"
sent1.split()
```

2. Sentence Tokenization
    

```python
# sentence tokenisation
sent2 = "I am going to Mumbai. I will stay there for 3 days. Let's hope the trip to be great"
sent2.split('.')
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730280429382/0d28c62f-4713-4b55-af43-1c07e60e924f.png align="center")

### 2\. Using Regular Expression

1. Word Tokenization
    

```python
# Word tokenisation
import re
sent3 = 'I am going to Delhi.'
tokens = re.findall("[\w']+", sent3)
tokens
```

2. Sentence Tokenization
    

```python
# sentence tokennisation
text = '''I'm going to Delhi. What are you doing Today?
We would love to have you there. Since, it is an holiday,
Please Join us!
Let's hope the trip to be great.
'''
sentences = re.compile('[.!?] ').split(text)
sentences
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730280566450/35587103-aee2-4476-a20e-63551c2b9947.png align="center")

### 3\. Using NLTK

```python
from nltk.tokenize import word_tokenize, sent_tokenize
import nltk
nltk.download('punkt')
```

```python
# word tokenize
sent1 = "I am going to travel Mumbai.
word_tokenize(sent1)
```

```python
# sentence tokenize
text = '''I'm going to Delhi. What are you doing Today?
We would love to have you there. Since, it is an holiday,
Please Join us!
Let's hope the trip to be great.
'''
sent_tokenize(text)
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730281012009/7f6f02f7-354a-4dc9-b804-b4ed0bb6976b.png align="center")

---

# Stemming

It is faster, thus sometimes produce compromising results.

```python
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
def stem_words(text):
  return " ".join([ps.stem(word) for word in text.split()])
```

```python
sample = "walk walks walking walked"
stem_words(sample)
# output
# walk walk walk walk
```

Here you will notice that not all the words makes sentence when using stemming data preprocessing technique.

```python
text = "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie"
print(text)
# Ouput 
# probabl my alltim favourit....
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730281656959/5aeb0eee-c084-4758-bb7e-43688c5a91a6.png align="center")

So we need something that converts the text into their root words and also makes sense. Thus, we came with the concept of lemmatization.

---

# Lemmatization

It is slower than stemmering but the results are way better.

```python
import nltk
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
wordnet_lemmatizer = WordNetLemmatizer()
#------------------------------------------
sentence = "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun"
punctuations = "?:!.,;"
sentence_words = nltk.word_tokenize(sentence)
for word in sentence_words:
  if word in punctuations:
    sentence_words.remove(word)
sentence_words
#------------------------------------------
print("{0:20}{1:20}".format("Word", "Lemma"))
for word in sentence_words:
  print("{0:20}{1:20}".format(word, wordnet_lemmatizer.lemmatize(word,pos='v')))
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730282466776/c3b04e8d-51ab-4ff4-972a-11b9c01f279a.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1730282484810/1b8a12e8-29ed-4fb2-9a3b-132cfdebd762.png align="center")

---

**üéâThat's a wrap for Day 4!**

Practice these and let me know if you have any doubts.  
In our next post, we'll learn data representation techniques. Stay tuned for more insights and exciting experiments!

**‚ñ∂ Next ‚Üí Day 5: Data Representation Techniques.**

**‚ñ∂ Full Playlist :** [**Master GenAI Series**](https://manavpaul.hashnode.dev/series/generative-ai)

üë®üèª‚Äçüíª Connect with me on [LinkedIn](https://www.linkedin.com/in/manav-paul/) and [X](https://x.com/themanavpaul)(Twitter).